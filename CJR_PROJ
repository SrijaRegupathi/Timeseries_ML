%matplotlib inline

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error

from statsmodels.tsa.statespace.sarimax import SARIMAX
from prophet import Prophet

import tensorflow as tf
from tensorflow.keras.layers import Input, LSTM, Dense, Attention, GlobalAveragePooling1D
from tensorflow.keras.models import Model

import keras_tuner as kt
import shutil

url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv"
data = pd.read_csv(url)

data.columns = ["Month", "Passengers"]
data["Month"] = pd.to_datetime(data["Month"])
data.set_index("Month", inplace=True)

np.random.seed(42)
data["Ticket_Price"] = np.linspace(100, 300, len(data)) + np.random.normal(0, 10, len(data))
data["Fuel_Cost"] = 200 + np.sin(np.arange(len(data)) / 6) * 20
data["Economic_Index"] = np.random.normal(50, 5, len(data))

data.head()

data.plot(subplots=True, figsize=(10, 8))
plt.suptitle("Multivariate Time Series Variables")
plt.show()

scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(data)

def create_sequences(data, window=12):
    X, y = [], []
    for i in range(len(data) - window):
        X.append(data[i:i+window])
        y.append(data[i+window, 0])
    return np.array(X), np.array(y)

WINDOW = 12
X, y = create_sequences(scaled_data, WINDOW)

X.shape, y.shape

def rolling_origin_split(X, y, n_splits=3):
    split_size = len(X) // (n_splits + 1)
    splits = []
    for i in range(n_splits):
        end = split_size * (i + 1)
        splits.append((X[:end], X[end:end+split_size], y[:end], y[end:end+split_size]))
    return splits

splits = rolling_origin_split(X, y)

train_size = int(len(data) * 0.8)
train, test = data.iloc[:train_size], data.iloc[train_size:]

sarima = SARIMAX(train["Passengers"], order=(1,1,1), seasonal_order=(1,1,1,12))
sarima_fit = sarima.fit(disp=False)

sarima_pred = sarima_fit.forecast(len(test))
sarima_rmse = np.sqrt(mean_squared_error(test["Passengers"], sarima_pred))

sarima_rmse

prophet_df = data.reset_index()[["Month", "Passengers"]]
prophet_df.columns = ["ds", "y"]

prophet_model = Prophet()
prophet_model.fit(prophet_df.iloc[:train_size])

future = prophet_model.make_future_dataframe(periods=len(test), freq="MS")
forecast = prophet_model.predict(future)

prophet_pred = forecast["yhat"].iloc[-len(test):].values
prophet_rmse = np.sqrt(mean_squared_error(test["Passengers"], prophet_pred))

prophet_rmse

def build_attention_model(units=64):
    inputs = Input(shape=(WINDOW, X.shape[2]))
    lstm_out = LSTM(units, return_sequences=True)(inputs)
    attention_out = Attention()([lstm_out, lstm_out])
    context = GlobalAveragePooling1D()(attention_out)
    output = Dense(1)(context)

    model = Model(inputs, output)
    model.compile(optimizer="adam", loss="mse")
    return model

shutil.rmtree("tuner", ignore_errors=True)

def tuner_model(hp):
    units = hp.Int("units", 32, 128, step=32)
    return build_attention_model(units)

tuner = kt.RandomSearch(
    tuner_model,
    objective="val_loss",
    max_trials=5,
    directory="tuner",
    project_name="attention_ts"
)

X_train, X_test, y_train, y_test = splits[0]

tuner.search(
    X_train,
    y_train,
    validation_data=(X_test, y_test),
    epochs=10,
    verbose=1
)

best_model = tuner.get_best_models(1)[0]
lstm_rmse = np.sqrt(best_model.evaluate(X_test, y_test))
lstm_rmse

lstm_pred = best_model.predict(X_test)

pmin = scaler.data_min_[0]
pmax = scaler.data_max_[0]

y_test_actual = y_test * (pmax - pmin) + pmin
y_pred_actual = lstm_pred.flatten() * (pmax - pmin) + pmin

plt.figure(figsize=(10,4))
plt.plot(y_test_actual, label="Actual")
plt.plot(y_pred_actual, label="Predicted")
plt.title("Actual vs Predicted Passenger Count (LSTM + Attention)")
plt.legend()
plt.show()

attention_model = Model(
    inputs=best_model.input,
    outputs=best_model.layers[2].output
)

attention_weights = attention_model.predict(X_test[:1])

plt.figure(figsize=(6,4))
plt.imshow(attention_weights[0], aspect="auto")
plt.colorbar()
plt.title("Attention Weight Heatmap")
plt.xlabel("Features")
plt.ylabel("Time Steps")
plt.show()

results = pd.DataFrame({
    "Model": ["SARIMA", "Prophet", "LSTM + Attention"],
    "RMSE": [sarima_rmse, prophet_rmse, lstm_rmse]
})

results

plt.figure(figsize=(6,4))
plt.bar(results["Model"], results["RMSE"])
plt.title("Model Performance Comparison (RMSE)")
plt.ylabel("RMSE")
plt.show()

